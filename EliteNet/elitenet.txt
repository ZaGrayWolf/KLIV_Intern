import torch
import torch.nn as nn
import torch.nn.functional as F
# No need for ptflops in this manual calculation version
# from ptflops import get_model_complexity_info

# --- Network Architecture Definitions (as provided) ---

class ConvBlockNoSkip(nn.Module):
    def __init__(self, in_c, out_c, k_sz=3, pool=True):
        super(ConvBlockNoSkip, self).__init__()
        pad = (k_sz - 1) // 2
        self.out_channels = out_c # Store out_channels for calculation tracing
        block = []
        if pool:
            self.pool = nn.MaxPool2d(kernel_size=2)
        else:
            self.pool = False

        # First Conv -> ReLU -> BN
        block.append(nn.Conv2d(in_c, out_c, kernel_size=k_sz, padding=pad, bias=True)) # Explicitly use bias=True for calculation
        block.append(nn.ReLU())
        block.append(nn.BatchNorm2d(out_c))

        # Second Conv -> ReLU -> BN
        block.append(nn.Conv2d(out_c, out_c, kernel_size=k_sz, padding=pad, bias=True)) # Explicitly use bias=True for calculation
        block.append(nn.ReLU())
        block.append(nn.BatchNorm2d(out_c))

        self.block = nn.Sequential(*block)

    def forward(self, x):
        if self.pool:
            x = self.pool(x)
        out = self.block(x)
        return out

# Keeping this definition for context, but the calculation focuses on UNetEncoderWithSkip
class UNetEncoderNoSkip(nn.Module):
    def __init__(self, in_c, layers, k_sz=3):
        """
        Encoder-only UNet model without skip connections.
        """
        super(UNetEncoderNoSkip, self).__init__()
        self.first = ConvBlockNoSkip(in_c=in_c, out_c=layers[0], k_sz=k_sz, pool=False)
        self.layers = layers
        self.down_path = nn.ModuleList()
        for i in range(len(layers) - 1):
            block = ConvBlockNoSkip(in_c=layers[i], out_c=layers[i + 1], k_sz=k_sz, pool=True)
            self.down_path.append(block)

    def forward(self, x):
        x = self.first(x)
        feature_maps = [x]
        for down in self.down_path:
            x = down(x)
            feature_maps.append(x)
        return feature_maps


class UNetEncoderWithSkip(nn.Module):
    def __init__(self, in_c, layers, k_sz=3):
        """
        UNet encoder with skip connections using ConvTranspose2d and addition.
        Note: This is not a standard UNet decoder structure (which uses concatenation).
        """
        super(UNetEncoderWithSkip, self).__init__()
        self.first = ConvBlockNoSkip(in_c=in_c, out_c=layers[0], k_sz=k_sz, pool=False)
        self.layers = layers
        self.down_path = nn.ModuleList()
        self.up_path = nn.ModuleList()
        # Encoder Down Path
        for i in range(len(layers) - 1):
            block = ConvBlockNoSkip(in_c=layers[i], out_c=layers[i + 1], k_sz=k_sz, pool=True)
            self.down_path.append(block)
        # Decoder Up Path (using ConvTranspose2d for upsampling)
        for i in range(len(layers) - 1):
             # ConvTranspose goes from deeper channels to shallower channels
            self.up_path.append(nn.ConvTranspose2d(layers[i+1], layers[i], kernel_size=2, stride=2, bias=True)) # Explicitly use bias=True

        # Added a final conv layer after the upsampling path
        self.final_conv = nn.Conv2d(layers[0], in_c, kernel_size=1, bias=True) # Explicitly use bias=True

    def forward(self, x):
        # Encoder
        x1 = self.first(x) # Output of the first block (no pool)
        feature_maps = [x1] # Store for skip connection

        x = x1 # Start processing down path from x1
        for i, down in enumerate(self.down_path):
            x = down(x) # down block includes pooling
            feature_maps.append(x) # Store output of each down block

        # Decoder
        x = feature_maps[-1] # Start decoder with the deepest feature map
        # Iterate through up_path in reverse order
        for i, up in enumerate(reversed(self.up_path)):
            x = up(x) # Upsample
            # Add skip connection: x + feature_maps[corresponding encoder level]
            # The encoder feature maps list includes the first block output (index 0)
            # followed by outputs of down_path[0], down_path[1], ...
            # up_path[-1] corresponds to down_path[-1] input level, connects to feature_maps[len(layers)-2]
            # up_path[-2] corresponds to down_path[-2] input level, connects to feature_maps[len(layers)-3]
            # etc. The index is len(layers) - 2 - i (where i is 0, 1, 2... in the reversed loop)
            skip_connection = feature_maps[len(self.layers) - 2 - i]
            x = x + skip_connection # Element-wise addition

        # Final Convolution
        x = self.final_conv(x)
        # The original code returned x, feature_maps. Let's keep it matching the provided forward signature.
        return x, feature_maps

# --- Theoretical Calculation Functions (Adapted and Corrected) ---
# Based on previous successful calculations and corrections for this model

def calculate_conv_flops_params(input_shape, output_channels, kernel_size, stride, padding, bias=True):
    """Calculates FLOPs (adds+mults) and Parameters for a standard Conv2d layer."""
    if len(input_shape) != 3:
        raise ValueError(f"Conv input shape must be (H, W, C), but got {input_shape}")
    in_channels = input_shape[2]
    k_h, k_w = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size
    s_h, s_w = (stride, stride) if isinstance(stride, int) else stride
    p_h, p_w = (padding, padding) if isinstance(padding, int) else padding

    out_height = (input_shape[0] + 2 * p_h - (k_h - 1) - 1) // s_h + 1
    out_width = (input_shape[1] + 2 * p_w - (k_w - 1) - 1) // s_w + 1

    if out_height <= 0 or out_width <= 0:
         # This indicates an issue with input size, kernel, stride, padding
         print(f"Warning: Conv input shape {input_shape}, kernel {kernel_size}, stride {stride}, padding {padding} resulted in non-positive output dimensions: ({out_height}, {out_width}). This layer is likely not reachable or configured correctly for this input. Returning 0 for this layer.")
         return 0, 0, (max(0, out_height), max(0, out_width), output_channels)

    params = (k_h * k_w * in_channels + (1 if bias else 0)) * output_channels

    # FLOPs: (kH * kW * in_channels) mults + (kH * kW * in_channels - 1) adds per output element + bias adds
    num_macs_per_output_channel = k_h * k_w * in_channels
    mults = num_macs_per_output_channel * out_height * out_width * output_channels
    adds = (num_macs_per_output_channel - 1) * out_height * out_width * output_channels if num_macs_per_output_channel > 0 else 0
    bias_adds = out_height * out_width * output_channels if bias else 0
    total_flops = mults + adds + bias_adds # Sum of all multiplications and additions

    return params, total_flops, (out_height, out_width, output_channels)

def calculate_bn_flops_params(input_shape):
    """Calculates FLOPs (adds+mults) and Parameters (gamma+beta) for a BatchNorm2d layer."""
    if len(input_shape) != 3:
        raise ValueError(f"BN input shape must be (H, W, C), but got {input_shape}")

    num_features = input_shape[2]
    params = 2 * num_features # gamma and beta are learnable

    total_elements = input_shape[0] * input_shape[1] * input_shape[2]
    flops = 2 * total_elements # 2 operations (mult + add) per element at inference

    return params, flops, input_shape # Output shape is the same

def calculate_relu_flops(input_shape):
    """Calculates FLOPs (comparisons/assignments) for a ReLU activation."""
    if len(input_shape) != 3:
        raise ValueError(f"ReLU input shape must be (H, W, C), but got {input_shape}")

    total_elements = input_shape[0] * input_shape[1] * input_shape[2]
    flops = total_elements # 1 operation per element

    return 0, flops, input_shape # No parameters, output shape is the same

def calculate_pooling_flops_params(input_shape, kernel_size, stride):
     # MaxPool has 0 parameters. FLOPs are comparisons, typically ignored in summaries.
    if len(input_shape) != 3:
        raise ValueError(f"Pool input shape must be (H, W, C), but got {input_shape}")

    k_h, k_w = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size
    s_h, s_w = (stride, stride) if isinstance(stride, int) else stride

    # Standard pooling output shape calculation (assuming integer division)
    out_height = (input_shape[0] - k_h) // s_h + 1
    out_width = (input_shape[1] - k_w) // s_w + 1

    return 0, 0, (out_height, out_width, input_shape[2]) # 0 params, 0 flops, updated shape

def calculate_convtranspose_flops_params(input_shape, output_channels, kernel_size, stride, padding, output_padding, bias=True):
    """Calculates FLOPs (adds+mults approximation) and Parameters for a ConvTranspose2d layer."""
    if len(input_shape) != 3:
        raise ValueError(f"ConvTranspose input shape must be (H, W, C), but got {input_shape}")
    in_channels = input_shape[2]
    k_h, k_w = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size
    s_h, s_w = (stride, stride) if isinstance(stride, int) else stride
    p_h, p_w = (padding, padding) if isinstance(padding, int) else padding
    op_h, op_w = (output_padding, output_padding) if isinstance(output_padding, int) else output_padding

    # Calculate output spatial dimensions for ConvTranspose2d
    out_height = (input_shape[0] - 1) * s_h - 2 * p_h + (k_h - 1) + op_h + 1
    out_width = (input_shape[1] - 1) * s_w - 2 * p_w + (k_w - 1) + op_w + 1

    if out_height <= 0 or out_width <= 0:
         print(f"Warning: ConvTranspose input shape {input_shape}, kernel {kernel_size}, stride {stride}, padding {padding}, output_padding {output_padding} resulted in non-positive output dimensions: ({out_height}, {out_width}). Returning 0 for this layer.")
         return 0, 0, (max(0, out_height), max(0, out_width), output_channels)

    # Parameters: (in_channels * kernel_height * kernel_width + bias) * output_channels
    # Corrected formula for ConvTranspose2d parameters
    params = (in_channels * k_h * k_w + (1 if bias else 0)) * output_channels

    # FLOPs: Approximately 2 * MACs. MACs = kH * kW * out_channels * H_in * W_in * in_channels
    # This count represents the total number of effective multiply-accumulate operations.
    # We use 2 * MACs to be roughly consistent with the Conv2d (adds + mults) count.
    macs = k_h * k_w * output_channels * input_shape[0] * input_shape[1] * in_channels
    total_flops = 2 * macs
    bias_adds = out_height * out_width * output_channels if bias else 0
    total_flops += bias_adds # Add bias additions

    return params, total_flops, (out_height, out_width, output_channels)

def calculate_addition_flops(input_shape):
    """Calculates FLOPs for element-wise addition."""
    if len(input_shape) != 3:
        raise ValueError(f"Addition input shape must be (H, W, C), but got {input_shape}")

    total_elements = input_shape[0] * input_shape[1] * input_shape[2]
    flops = total_elements # 1 addition per element

    return 0, flops, input_shape # No parameters, output shape is the same


def trace_unet_with_skip_flops_params(model, input_res, in_ch, out_ch, layers, k_sz=3):  # H x W
    """Calculates total theoretical FLOPs and Parameters for the UNetEncoderWithSkip architecture by tracing."""
    total_params = 0
    total_flops = 0

    # Initial input shape: (Height, Width, Channels)
    # Model expects (C, H, W), our calculations use (H, W, C) shape tuple
    curr_shape = (input_res[0], input_res[1], in_ch)
    print(f"Input shape (H, W, C): {curr_shape}")

    # List to store shapes (H, W, C) of encoder feature maps AFTER BN+ReLU but BEFORE pooling
    encoder_skip_shapes = []

    # --- Encoder (Tracing forward pass) ---
    print("\n--- Encoder ---")

    # First block: model.first (ConvBlockNoSkip with pool=False)
    print("Block: model.first (no pool)")
    # Contains: Conv2d -> ReLU -> BatchNorm2d -> Conv2d -> ReLU -> BatchNorm2d
    conv_block = model.first.block # Access the sequential block

    # Step through layers in the first ConvBlockNoSkip
    layer_names = ['Conv1', 'ReLU1', 'BN1', 'Conv2', 'ReLU2', 'BN2']
    for i, module in enumerate(conv_block):
        layer_name = layer_names[i]
        if isinstance(module, nn.Conv2d):
            params, flops, curr_shape = calculate_conv_flops_params(
                curr_shape, module.out_channels, module.kernel_size[0], module.stride[0], module.padding[0], bias=module.bias is not None
            )
            total_params += params
            total_flops += flops
            print(f"  - {layer_name}: Output shape: {curr_shape}, Params: {params}, FLOPs: {flops}")
        elif isinstance(module, nn.ReLU):
            params, flops, curr_shape = calculate_relu_flops(curr_shape) # relu has 0 params
            total_flops += flops
            print(f"  - {layer_name}: Output shape: {curr_shape}, Params: {params}, FLOPs: {flops}")
        elif isinstance(module, nn.BatchNorm2d):
            params, flops, curr_shape = calculate_bn_flops_params(curr_shape)
            total_params += params
            total_flops += flops
            print(f"  - {layer_name}: Output shape: {curr_shape}, Params: {params}, FLOPs: {flops}")

    encoder_skip_shapes.append(curr_shape) # Save shape after first block (before any pooling)

    # Down Path blocks: model.down_path (ConvBlockNoSkip with pool=True)
    for i, down_block_module in enumerate(model.down_path):
        print(f"Block: model.down_path[{i}] (pool=True)")
        # Contains: MaxPool2d -> Conv2d -> ReLU -> BatchNorm2d -> Conv2d -> ReLU -> BatchNorm2d
        pool_layer = down_block_module.pool
        conv_block = down_block_module.block # Access the sequential block

        # Max Pooling
        params, flops, curr_shape_after_pool = calculate_pooling_flops_params(
             curr_shape, pool_layer.kernel_size, pool_layer.stride
        )
        total_params += params # Should be 0
        total_flops += flops # Should be 0 (by convention)
        print(f"  - MaxPool: Input shape: {curr_shape}, Output shape: {curr_shape_after_pool}, Params: {params}, FLOPs: {flops} (ignored)")
        curr_shape = curr_shape_after_pool # Update shape after pooling

        # Step through layers in the ConvBlockNoSkip after pooling
        layer_names = ['Conv1', 'ReLU1', 'BN1', 'Conv2', 'ReLU2', 'BN2'] # Reset names for block layers
        for j, module in enumerate(conv_block):
            layer_name = layer_names[j]
            if isinstance(module, nn.Conv2d):
                 params, flops, curr_shape = calculate_conv_flops_params(
                     curr_shape, module.out_channels, module.kernel_size[0], module.stride[0], module.padding[0], bias=module.bias is not None
                 )
                 total_params += params
                 total_flops += flops
                 print(f"  - {layer_name}: Output shape: {curr_shape}, Params: {params}, FLOPs: {flops}")
            elif isinstance(module, nn.ReLU):
                 params, flops, curr_shape = calculate_relu_flops(curr_shape) # relu has 0 params
                 total_flops += flops
                 print(f"  - {layer_name}: Output shape: {curr_shape}, Params: {params}, FLOPs: {flops}")
            elif isinstance(module, nn.BatchNorm2d):
                 params, flops, curr_shape = calculate_bn_flops_params(curr_shape)
                 total_params += params
                 total_flops += flops
                 print(f"  - {layer_name}: Output shape: {curr_shape}, Params: {params}, FLOPs: {flops}")

        encoder_skip_shapes.append(curr_shape) # Save shape AFTER this down block (used for skip)


    # --- Decoder (Tracing forward pass) ---
    print("\n--- Decoder ---")

    # The deepest feature map is the output of the last down block
    curr_shape = encoder_skip_shapes[-1] # Start decoder path with the deepest shape

    # Up Path blocks: model.up_path (ConvTranspose2d) + Addition
    # Iterate through up_path in reverse order, corresponding to down_path in forward order
    # up_path[0] upsamples layers[1]->layers[0] (connects to encoder_skip_shapes[0])
    # up_path[1] upsamples layers[2]->layers[1] (connects to encoder_skip_shapes[1])
    # ...
    # up_path[-1] upsamples layers[-1]->layers[-2] (connects to encoder_skip_shapes[-2])

    num_up_layers = len(model.up_path)
    for i in range(num_up_layers):
        up_layer = model.up_path[num_up_layers - 1 - i] # Get up_path layer in reverse order
        skip_shape = encoder_skip_shapes[num_up_layers - 1 - i] # Get corresponding encoder skip shape

        print(f"Up Block: model.up_path[{num_up_layers - 1 - i}]")

        # ConvTranspose2d (Upsampling)
        # Output size matches the spatial dimensions of the corresponding skip connection
        # Input shape is curr_shape (output of previous decoder step or deepest encoder)
        params, flops, curr_shape_after_up = calculate_convtranspose_flops_params(
            curr_shape, up_layer.out_channels, up_layer.kernel_size[0], up_layer.stride[0], up_layer.padding[0], up_layer.output_padding[0], bias=up_layer.bias is not None
        )
        total_params += params
        total_flops += flops
        # Verify spatial size matches skip connection
        if curr_shape_after_up[0] != skip_shape[0] or curr_shape_after_up[1] != skip_shape[1]:
            print(f"Error: Upsample output shape {curr_shape_after_up} does not match skip shape {skip_shape} spatially for addition!")
            # Adjust shape to match skip for subsequent addition/conv calculations
            curr_shape_after_up = (skip_shape[0], skip_shape[1], curr_shape_after_up[2])

        print(f"  - ConvTranspose2d: Input shape: {curr_shape}, Output shape: {curr_shape_after_up}, Params: {params}, FLOPs: {flops}")
        curr_shape = curr_shape_after_up # Update shape after upsampling

        # Addition (Skip Connection)
        # Inputs are curr_shape (upsampled) and skip_shape (from encoder)
        # They must have the same shape (H, W, C) for element-wise addition
        if curr_shape != skip_shape:
             print(f"Error: Shape mismatch {curr_shape} vs {skip_shape} for addition!")
             # Attempt to proceed using the shape after upsampling if mismatch happens
             addition_shape = curr_shape
        else:
             addition_shape = curr_shape

        params, flops, curr_shape_after_add = calculate_addition_flops(addition_shape) # Add has 0 params
        total_flops += flops
        print(f"  - Addition (Skip): Input shape: {addition_shape}, Output shape: {curr_shape_after_add}, Params: {params}, FLOPs: {flops}")
        curr_shape = curr_shape_after_add # Shape remains the same after addition


    # --- Final Convolution Layer ---
    print("\n--- Final Convolution ---")
    # Input to final_conv is the output of the last addition
    final_conv_layer = model.final_conv
    params, flops, curr_shape = calculate_conv_flops_params(
        curr_shape, final_conv_layer.out_channels, final_conv_layer.kernel_size[0], final_conv_layer.stride[0], final_conv_layer.padding[0], bias=final_conv_layer.bias is not None
    )
    total_params += params
    total_flops += flops
    print(f"Layer final_conv: Input shape: {encoder_skip_shapes[0]} (after last add), Output shape: {curr_shape}, Params: {params}, FLOPs: {flops}")


    # --- Final Results ---
    print("\n--- Summary ---")
    print(f"Input Resolution (HxW): {input_res}")
    print(f"Input Channels: {in_ch}")
    print(f"Output Channels (final_conv): {out_ch}") # Note: final_conv output channels determine this
    print(f"Total Parameters: {total_params / 1e6:.4f} Million")
    # Using BFLOPs (Billion FLOPs)
    print(f"Total FLOPs: {total_flops / 1e9:.4f} Billion (based on adds + mults, ignoring MaxPool)")
    print("-" * 40) # Separator


# --- Benchmarking Setup ---

def run_manual_benchmarks():
    input_spatial_sizes = [
        (360, 640),    # H, W
        (720, 1280),   # H, W
        (760, 1360),   # H, W
        (900, 1600),   # H, W
        (1080, 1920),  # H, W
        (1152, 2048),  # H, W
        (1440, 2560),  # H, W
    ]
    in_channels = 3
    # The UNetEncoderWithSkip's final conv outputs 'in_c' channels, not 'out_ch'.
    # Let's adjust the calculation call to match the model's structure.
    # Or assume the user *intended* the final layer to output 'out_ch'.
    # Based on the previous SUMNet, let's assume the *final* layer outputs 'out_ch'.
    # We'll pass 'out_ch' to the function, and the function will use the model's final_conv.out_channels
    # which is 'in_c' in the current definition. Let's add an 'actual_final_out_ch' parameter
    # to the tracing function to make it explicit.
    actual_final_out_channels = in_channels # Based on the UNetEncoderWithSkip definition

    # Define the number of channels per level in the encoder
    layers = [4, 8, 16, 32] # Example layer structure

    print("--- Running UNetEncoderWithSkip Complexity Benchmarks (Manual Trace) ---")
    print(f"Encoder Layers Channels: {layers}")
    print(f"Fixed Input Channels: {in_channels}")
    print(f"Actual Final Output Channels (from model): {actual_final_out_channels}") # Based on model.final_conv
    print("-" * 40)

    # Instantiate the model once, as its structure doesn't change, only the input size affects FLOPs/spatial shapes
    # We pass the model instance to the tracing function
    model = UNetEncoderWithSkip(in_c=in_channels, layers=layers, k_sz=3)


    for res in input_spatial_sizes:
        print(f"\n--- Calculating for Input Resolution: {res[0]}x{res[1]} ---")
        # Call the tracing function for each resolution
        trace_unet_with_skip_flops_params(
            model,
            input_res=res,
            in_ch=in_channels,
            out_ch=actual_final_out_channels, # Pass the actual final output channels
            layers=layers,
            k_sz=3
        )


    print("--- Benchmarks Complete ---")


# --- Main Execution ---
if __name__ == "__main__":
    run_manual_benchmarks()